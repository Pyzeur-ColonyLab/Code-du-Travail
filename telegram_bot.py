#!/usr/bin/env python3
"""
Telegram Bot for Code du Travail - Mistral 7B Fine-tuned Model

This bot integrates a fine-tuned Mistral 7B model with Telegram to answer
questions about French labor law (Code du Travail).
"""

import os
import logging
import asyncio
from typing import Optional
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
from peft import PeftModel, PeftConfig
from telegram import Update
from telegram.ext import Application, CommandHandler, MessageHandler, filters, ContextTypes
from dotenv import load_dotenv
import psutil
import time

# Load environment variables
load_dotenv()

# Configure logging
logging.basicConfig(
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    level=logging.INFO,
    handlers=[
        logging.FileHandler('bot.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class MistralTelegramBot:
    def __init__(self):
        self.model = None
        self.tokenizer = None
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.model_name = "Pyzeur/Code-du-Travail-mistral-finetune"
        self.base_model_name = "mistralai/Mistral-7B-Instruct-v0.1"  # Base model for LoRA
        self.max_length = 2048
        self.is_loading = False
        
        # Get environment variables
        self.telegram_token = os.getenv('TELEGRAM_BOT_TOKEN')
        if not self.telegram_token:
            raise ValueError("TELEGRAM_BOT_TOKEN environment variable is required")
            
        logger.info(f"Initializing bot with device: {self.device}")
        
    async def load_model(self):
        """Load the fine-tuned LoRA model and tokenizer"""
        if self.model is not None:
            return
            
        self.is_loading = True
        logger.info("Loading LoRA model and tokenizer...")
        
        try:
            # Get authentication token if available
            use_auth_token = os.getenv('HUGGING_FACE_TOKEN')
            if use_auth_token:
                logger.info("Using HuggingFace authentication token")
            
            # Configure quantization for memory efficiency
            if self.device == "cuda":
                quantization_config = BitsAndBytesConfig(
                    load_in_4bit=True,
                    bnb_4bit_compute_dtype=torch.float16,
                    bnb_4bit_use_double_quant=True,
                    bnb_4bit_quant_type="nf4"
                )
            else:
                quantization_config = None
            
            # Load tokenizer from the LoRA adapter (it should use the base model's tokenizer)
            logger.info("Loading tokenizer...")
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.base_model_name,  # Use base model for tokenizer
                trust_remote_code=True,
                token=use_auth_token
            )
            
            # Add pad token if it doesn't exist
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            
            # Load base model
            logger.info("Loading base model...")
            base_model = AutoModelForCausalLM.from_pretrained(
                self.base_model_name,
                quantization_config=quantization_config,
                device_map="auto" if self.device == "cuda" else None,
                torch_dtype=torch.float16 if self.device == "cuda" else torch.float32,
                trust_remote_code=True,
                token=use_auth_token
            )
            
            # Load LoRA adapter
            logger.info("Loading LoRA adapter...")
            self.model = PeftModel.from_pretrained(
                base_model,
                self.model_name,
                token=use_auth_token
            )
            
            if self.device == "cpu":
                self.model = self.model.to(self.device)
            
            logger.info("LoRA model loaded successfully!")
            
        except Exception as e:
            logger.error(f"Error loading model: {e}")
            # Fallback to base model only
            logger.info("Falling back to base model without LoRA...")
            try:
                self.model = AutoModelForCausalLM.from_pretrained(
                    self.base_model_name,
                    quantization_config=quantization_config,
                    device_map="auto" if self.device == "cuda" else None,
                    torch_dtype=torch.float16 if self.device == "cuda" else torch.float32,
                    trust_remote_code=True,
                    token=use_auth_token
                )
                if self.device == "cpu":
                    self.model = self.model.to(self.device)
                logger.info("Base model loaded successfully (without fine-tuning)")
            except Exception as fallback_error:
                logger.error(f"Error loading fallback model: {fallback_error}")
                raise
        finally:
            self.is_loading = False
    
    def generate_response(self, question: str) -> str:
        """Generate response using the fine-tuned model"""
        if self.model is None or self.tokenizer is None:
            return "‚ùå Le mod√®le n'est pas encore charg√©. Veuillez patienter..."
        
        try:
            # Format the prompt for Mistral
            prompt = f"<s>[INST] {question} [/INST]"
            
            # Tokenize input
            inputs = self.tokenizer(
                prompt,
                return_tensors="pt",
                truncation=True,
                max_length=self.max_length // 2
            ).to(self.device)
            
            # Generate response
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=512,
                    do_sample=True,
                    temperature=0.7,
                    top_p=0.9,
                    top_k=50,
                    repetition_penalty=1.1,
                    pad_token_id=self.tokenizer.eos_token_id,
                    eos_token_id=self.tokenizer.eos_token_id
                )
            
            # Decode response
            response = self.tokenizer.decode(
                outputs[0][inputs.input_ids.shape[1]:],
                skip_special_tokens=True
            ).strip()
            
            # Clean up response
            if not response:
                response = "Je n'ai pas pu g√©n√©rer une r√©ponse appropri√©e √† votre question."
            
            return response
            
        except Exception as e:
            logger.error(f"Error generating response: {e}")
            return f"‚ùå Erreur lors de la g√©n√©ration de la r√©ponse: {str(e)}"
    
    def get_system_info(self) -> str:
        """Get system information"""
        try:
            cpu_percent = psutil.cpu_percent(interval=1)
            memory = psutil.virtual_memory()
            disk = psutil.disk_usage('/')
            
            info = f"üìä **Informations syst√®me:**\n\n"
            info += f"üñ•Ô∏è CPU: {cpu_percent}%\n"
            info += f"üíæ RAM: {memory.percent}% ({memory.used // (1024**3)}GB / {memory.total // (1024**3)}GB)\n"
            info += f"üíø Disque: {disk.percent}% ({disk.used // (1024**3)}GB / {disk.total // (1024**3)}GB)\n"
            info += f"üîß Device: {self.device.upper()}\n"
            
            if torch.cuda.is_available():
                gpu_memory = torch.cuda.get_device_properties(0).total_memory // (1024**3)
                gpu_used = torch.cuda.memory_allocated(0) // (1024**3)
                info += f"üéÆ GPU: {gpu_used}GB / {gpu_memory}GB\n"
            
            model_type = "Non charg√©"
            if self.model:
                if hasattr(self.model, 'peft_config'):
                    model_type = "LoRA Fine-tun√©"
                else:
                    model_type = "Base Model"
            info += f"ü§ñ Mod√®le: {model_type}\n"
            
            return info
            
        except Exception as e:
            return f"‚ùå Erreur lors de la r√©cup√©ration des informations syst√®me: {str(e)}"

# Initialize bot instance
bot_instance = MistralTelegramBot()

# Command handlers
async def start(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    """Send a message when the command /start is issued."""
    welcome_message = (
        "üëã Bonjour ! Je suis votre assistant virtuel sp√©cialis√© dans le Code du Travail fran√ßais.\n\n"
        "üíº Posez-moi vos questions sur le droit du travail, les cong√©s, les contrats, "
        "les proc√©dures de licenciement, et bien plus encore !\n\n"
        "üìù **Commandes disponibles:**\n"
        "/start - Afficher ce message\n"
        "/help - Aide et informations\n"
        "/status - √âtat du syst√®me\n\n"
        "‚ú® Envoyez-moi simplement votre question et je vous r√©pondrai !"
    )
    
    await update.message.reply_text(welcome_message, parse_mode='Markdown')
    
    # Load model if not already loaded
    if bot_instance.model is None and not bot_instance.is_loading:
        await update.message.reply_text("üîÑ Chargement du mod√®le en cours, veuillez patienter...")
        await bot_instance.load_model()
        await update.message.reply_text("‚úÖ Mod√®le charg√© ! Vous pouvez maintenant poser vos questions.")

async def help_command(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    """Send a message when the command /help is issued."""
    help_text = (
        "üÜò **Aide - Bot Code du Travail**\n\n"
        "Ce bot utilise un mod√®le Mistral 7B fine-tun√© sp√©cialement pour r√©pondre "
        "aux questions sur le Code du Travail fran√ßais.\n\n"
        "üìã **Comment utiliser le bot:**\n"
        "‚Ä¢ Posez vos questions directement\n"
        "‚Ä¢ Soyez pr√©cis dans vos demandes\n"
        "‚Ä¢ Le bot peut traiter des sujets comme:\n"
        "  - Contrats de travail\n"
        "  - Cong√©s et RTT\n"
        "  - Licenciements\n"
        "  - Temps de travail\n"
        "  - Salaires et primes\n"
        "  - Relations sociales\n\n"
        "‚ö†Ô∏è **Avertissement:** Les r√©ponses sont fournies √† titre informatif. "
        "Pour des conseils juridiques pr√©cis, consultez un avocat sp√©cialis√©."
    )
    
    await update.message.reply_text(help_text, parse_mode='Markdown')

async def status(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    """Show system status"""
    status_info = bot_instance.get_system_info()
    await update.message.reply_text(status_info, parse_mode='Markdown')

async def handle_message(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    """Handle incoming messages and generate responses"""
    user_message = update.message.text
    user_id = update.effective_user.id
    username = update.effective_user.username or "Unknown"
    
    logger.info(f"Received message from {username} ({user_id}): {user_message[:100]}...")
    
    # Check if model is loading
    if bot_instance.is_loading:
        await update.message.reply_text(
            "üîÑ Le mod√®le est en cours de chargement, veuillez patienter quelques instants..."
        )
        return
    
    # Load model if not loaded
    if bot_instance.model is None:
        await update.message.reply_text("üîÑ Chargement du mod√®le...")
        try:
            await bot_instance.load_model()
            await update.message.reply_text("‚úÖ Mod√®le charg√© !")
        except Exception as e:
            await update.message.reply_text(f"‚ùå Erreur lors du chargement du mod√®le: {str(e)}")
            return
    
    # Show typing indicator
    await context.bot.send_chat_action(chat_id=update.effective_chat.id, action="typing")
    
    # Generate response
    start_time = time.time()
    response = bot_instance.generate_response(user_message)
    end_time = time.time()
    
    logger.info(f"Generated response in {end_time - start_time:.2f}s for user {username}")
    
    # Send response
    try:
        await update.message.reply_text(response)
    except Exception as e:
        logger.error(f"Error sending response: {e}")
        await update.message.reply_text(
            "‚ùå D√©sol√©, une erreur s'est produite lors de l'envoi de la r√©ponse."
        )

async def error_handler(update: object, context: ContextTypes.DEFAULT_TYPE) -> None:
    """Log errors caused by Updates."""
    logger.error(f"Update {update} caused error {context.error}")

def main() -> None:
    """Start the bot."""
    # Create the Application
    application = Application.builder().token(bot_instance.telegram_token).build()
    
    # Register handlers
    application.add_handler(CommandHandler("start", start))
    application.add_handler(CommandHandler("help", help_command))
    application.add_handler(CommandHandler("status", status))
    application.add_handler(MessageHandler(filters.TEXT & ~filters.COMMAND, handle_message))
    
    # Register error handler
    application.add_error_handler(error_handler)
    
    # Start the bot
    logger.info("Starting Telegram bot...")
    application.run_polling(allowed_updates=Update.ALL_TYPES)

if __name__ == '__main__':
    main()
